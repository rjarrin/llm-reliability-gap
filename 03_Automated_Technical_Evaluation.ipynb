{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3: Automated Technical Evaluation & Runtime Instrumentation\n",
        "\n",
        "### **Overview**\n",
        "This notebook contains the complete execution pipeline for **Phase 3** (as detailed in Section 3.4 of the methodology). It ingests the 1195 fully synthesized C programming solutions generated in Phase 2 and subjects them to a rigorous, multi-dimensional technical audit.\n",
        "\n",
        "### **Methodology**\n",
        "To identify the \"Technical Reliability Gap\" between syntactic adherence and logical correctness, this pipeline automatically evaluates each code generation using the following tools:\n",
        "* **Static Analysis:** `lizard` (v1.20.0) for Cyclomatic Complexity (CCN) and `cppcheck` (v.2.7-1) for Static Error Density (SED).\n",
        "* **Stylistic Adherence:** `clang-format` (LLVM standard) to identify formatting deviations.\n",
        "* **Dynamic Compilation:** `gcc` (v.11.4.0) with strict academic flags (`-Wall -Werror -std=c11`) to calculate the Compilation Success Rate (CSR).\n",
        "* **Memory & Runtime Auditing:** `Valgrind` to detect \"Definitely Lost\" heap memory leaks, and `UBSan` to catch fatal Undefined Behaviors (UB) during execution.\n",
        "\n",
        "**Target Output:** A consolidated CSV dataset containing the technical metrics and failure profiles for all 1195 iterations, which directly populates the tables and figures in the study.\n",
        "\n",
        "---\n",
        "**Note for Double-Blind Review:** This codebase relies on native Linux utilities (GCC, Valgrind) which are readily available in the Google Colab environment. If running locally, ensure these dependencies are installed via your system's package manager."
      ],
      "metadata": {
        "id": "mIkNH3C69l8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Environment Initialization & Data Validation\n",
        "This cell initializes the Python environment and establishes a persistent connection to the dataset generated in Phase 2. It mounts the Google Drive filesystem, verifies the target directory paths, and performs a preliminary audit to confirm all JSONL files are present and correctly named before the technical evaluation begins."
      ],
      "metadata": {
        "id": "VbR8c0eg9yj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- ENVIRONMENT INITIALIZATION ---\n",
        "# ==========================================\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "\n",
        "def setup_research_environment(target_folder):\n",
        "    \"\"\"\n",
        "    Mounts the Google Drive filesystem and validates the data directory.\n",
        "    Objective: Establish a persistent connection to the model-generated datasets.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(\"/content/drive\"):\n",
        "        print(\"üîç Attempting to mount Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    data_path = os.path.join(\"/content/drive/MyDrive/\", target_folder)\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        raise FileNotFoundError(f\"Critical Failure: Directory not found at {data_path}\")\n",
        "\n",
        "    print(f\"Research environment verified. Path set to: {data_path}\")\n",
        "    return data_path\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "FOLDER_NAME = \"CANAI_LLM_Results/Eval/\"\n",
        "\n",
        "try:\n",
        "    path = setup_research_environment(FOLDER_NAME)\n",
        "\n",
        "    # --- VALIDATION 1: FILENAME AUDIT ---\n",
        "    files = sorted([f for f in os.listdir(path) if f.endswith('.jsonl')])\n",
        "    print(f\"Validation Check: {len(files)} JSONL files detected.\\n\")\n",
        "\n",
        "    print(\"Detected Filenames:\")\n",
        "    for i, fname in enumerate(files, 1):\n",
        "        print(f\"  {i}. {fname}\")\n",
        "\n",
        "    if len(files) == 0:\n",
        "        print(\"Warning: No data files found. Please verify the folder contents.\")\n",
        "except Exception as e:\n",
        "    print(f\"Initialization Error: {e}\")"
      ],
      "metadata": {
        "id": "z7f3WruH9wxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data Parsing and Extraction Audit\n",
        "To prepare the dataset for static and dynamic analysis, the raw JSONL strings must be parsed to isolate the functional C code.\n",
        "\n",
        "This cell performs three critical preprocessing tasks:\n",
        "1. **Trace Sanitization:** It removes internal \"Chain-of-Thought\" (CoT) reasoning traces (e.g., `<think>` tags) generated by latent reasoning models to prevent metric pollution during static analysis.\n",
        "2. **Code Extraction:** It uses regex to isolate the terminal ` ```c ` markdown block from Step 2 of the prompt chain, representing the model's final synthesized solution.\n",
        "3. **Integrity Auditing:** It identifies and logs any iterations that failed to follow the formatting constraints (resulting in the 5 excluded iterations mentioned in the methodology, finalizing the dataset at 1195 items)."
      ],
      "metadata": {
        "id": "BtOg1Mxb9-nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- DATA PARSING & EXTRACTION AUDIT ---\n",
        "# ==========================================\n",
        "\n",
        "def clean_model_output(text):\n",
        "    \"\"\"\n",
        "    Removes internal reasoning traces (e.g., <think> tags) to prevent metric pollution.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
        "\n",
        "def extract_c_source_with_reason(text):\n",
        "    \"\"\"\n",
        "    Isolates the final C code block and provides a reason for any formatting failure.\n",
        "    Returns: (extracted_code, failure_reason)\n",
        "    \"\"\"\n",
        "    # Locates all C code blocks within Markdown delimiters\n",
        "    code_blocks = re.findall(r\"```c\\s*(.*?)\\s*```\", text, re.DOTALL)\n",
        "\n",
        "    if not code_blocks:\n",
        "        return \"\", \"Missing Delimiters (```c ... ```)\"\n",
        "\n",
        "    # Selects the terminal block to capture the final solution logic\n",
        "    final_block = code_blocks[-1].strip()\n",
        "    if not final_block:\n",
        "        return \"\", \"Empty Code Block (Delimiters found but no content)\"\n",
        "\n",
        "    return final_block, None\n",
        "\n",
        "def parse_metadata(filename):\n",
        "    \"\"\"\n",
        "    Extracts provenance metadata based on the verified Phase 2 naming convention:\n",
        "    e.g., SOLVED_YYYYMMDD_ModelName_TopicName_B1.jsonl\n",
        "    \"\"\"\n",
        "    pattern = r\"SOLVED_\\d{8}_(.+?)_(.+)_B\\d+\\.jsonl\"\n",
        "    match = re.match(pattern, filename)\n",
        "\n",
        "    if match:\n",
        "        return {\n",
        "            \"model\": match.group(1).replace(\"_\", \"/\"),\n",
        "            \"topic\": match.group(2).replace(\"_\", \" \")\n",
        "        }\n",
        "    return None\n",
        "\n",
        "# --- PROCESSING & EXTRACTION VALIDATION ---\n",
        "dataset = defaultdict(lambda: defaultdict(list))\n",
        "extraction_audit = {\"success\": 0, \"failed\": 0, \"unparsed_files\": 0}\n",
        "failure_log = []\n",
        "\n",
        "# Sorted processing for consistent indexing (uses 'path' from Cell 1)\n",
        "files = sorted([f for f in os.listdir(path) if f.endswith('.jsonl')])\n",
        "\n",
        "for fname in files:\n",
        "    meta = parse_metadata(fname)\n",
        "    if not meta:\n",
        "        extraction_audit[\"unparsed_files\"] += 1\n",
        "        continue\n",
        "\n",
        "    with open(os.path.join(path, fname), 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                it_id = data.get(\"iteration\", \"N/A\")\n",
        "                raw_steps = data.get(\"steps\", {})\n",
        "\n",
        "                # Cleaning traces and isolating code from Step 2\n",
        "                solution_text = clean_model_output(raw_steps.get(\"step_2\", \"\"))\n",
        "                extracted_code, reason = extract_c_source_with_reason(solution_text)\n",
        "\n",
        "                if extracted_code:\n",
        "                    extraction_audit[\"success\"] += 1\n",
        "                    data[\"extracted_code\"] = extracted_code\n",
        "                    dataset[meta['model']][meta['topic']].append(data)\n",
        "                else:\n",
        "                    extraction_audit[\"failed\"] += 1\n",
        "                    # Detailed logging of the extraction failure location\n",
        "                    failure_log.append({\n",
        "                        \"File\": fname,\n",
        "                        \"Iteration\": it_id,\n",
        "                        \"Model\": meta['model'],\n",
        "                        \"Topic\": meta['topic'],\n",
        "                        \"Reason\": reason\n",
        "                    })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"JSON Parse Error in {fname}: {e}\")\n",
        "\n",
        "# --- VALIDATION REPORT OUTPUT ---\n",
        "print(\"EXTRACTION VALIDATION REPORT\")\n",
        "print(\"-\" * 115)\n",
        "print(f\"Successfully Isolated C Code: {extraction_audit['success']}\")\n",
        "print(f\"Failed to Extract Code:     {extraction_audit['failed']}\")\n",
        "print(f\"Files Skipping (No Match):  {extraction_audit['unparsed_files']}\")\n",
        "print(\"-\" * 115)\n",
        "\n",
        "if failure_log:\n",
        "    print(\"\\nFAILURE PINPOINT LOG (Excluded from technical evaluation)\")\n",
        "    # Formatted table for clear mapping of failures to source files\n",
        "    header = f\"{'FILENAME':<45} | {'ITER':<5} | {'TOPIC':<30} | {'REASON':<25}\"\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    for fail in failure_log:\n",
        "        print(f\"{fail['File'][:45]:<45} | {fail['Iteration']:<5} | {fail['Topic'][:30]:<30} | {fail['Reason']:<25}\")\n",
        "    print(\"-\" * len(header))\n",
        "    print(f\"\\nVerification Note: These {extraction_audit['failed']} iterations lack valid C code blocks and are excluded from the dataset.\")\n",
        "else:\n",
        "    print(\"\\nAll extractions successful. No failures found.\")"
      ],
      "metadata": {
        "id": "dflgElLpNX8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Static Analysis: Structural Complexity (Lizard)\n",
        "This cell executes the first phase of the static \"Code-as-Text\" audit detailed in Section 3.4. It utilizes the `lizard` library (v1.20.0) to analyze the structural density of the generated C solutions.\n",
        "\n",
        "**Key Metrics Extracted:**\n",
        "* **Max/Avg Cyclomatic Complexity (CCN):** Quantifies the number of independent branching paths. This metric is used to establish the \"Reliability Ceiling\" for each architecture (RQ1).\n",
        "* **Token Count & NLOC:** Measures model verbosity. When cross-referenced with CCN, this generates the \"Logic Density\" (CCN per 100 Tokens) metric used to evaluate architectural efficiency between sparse MoE and dense models (RQ2).\n",
        "\n",
        "*(Note: System-level dependencies like `cppcheck` and `clang-format` are also installed in this cell to prepare the environment for the subsequent style and security audits).*"
      ],
      "metadata": {
        "id": "eFfZBYXrNYYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- EXPANDED COMPLEXITY (CCN) ANALYSIS ---\n",
        "# ==========================================\n",
        "print(\"Installing Static Analysis Tools...\")\n",
        "# Install native Linux tools required for subsequent cells\n",
        "!apt-get update -qq\n",
        "!apt-get install -y cppcheck clang-format -qq\n",
        "# Install lizard matching the v1.20.0 specification from the methodology\n",
        "!pip install lizard -q\n",
        "\n",
        "import lizard\n",
        "import tempfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calculate_complexity_expanded(dataset):\n",
        "    \"\"\"\n",
        "    Analyzes deep structural metrics using Lizard at the file level.\n",
        "    Captures Max/Avg CCN, Token Count, Parameter Count, and total NLOC.\n",
        "    \"\"\"\n",
        "    print(\"Analyzing Logical Complexity (Expanded Metrics)...\")\n",
        "    stats = {\"processed\": 0, \"skipped\": 0}\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "        tmp_path = os.path.join(tmp_dir, \"analysis_target.c\")\n",
        "\n",
        "        for model in dataset:\n",
        "            for topic in dataset[model]:\n",
        "                for entry in dataset[model][topic]:\n",
        "                    code = entry.get(\"extracted_code\", \"\")\n",
        "                    metrics = entry.get(\"metrics_static\", {})\n",
        "\n",
        "                    # Handle missing or empty code blocks\n",
        "                    if not code:\n",
        "                        entry[\"metrics_static\"] = {\n",
        "                            \"max_ccn\": 0, \"avg_ccn\": 0, \"tokens\": 0,\n",
        "                            \"avg_params\": 0, \"loc\": 0\n",
        "                        }\n",
        "                        stats[\"skipped\"] += 1\n",
        "                        continue\n",
        "\n",
        "                    # Write target code to temporary file for Lizard analysis\n",
        "                    with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                        f.write(code)\n",
        "\n",
        "                    try:\n",
        "                        analysis = lizard.analyze_file(tmp_path)\n",
        "\n",
        "                        if analysis.function_list:\n",
        "                            # Aggregate metrics across all functions in the file\n",
        "                            ccn_list = [f.cyclomatic_complexity for f in analysis.function_list]\n",
        "                            param_list = [len(f.parameters) for f in analysis.function_list]\n",
        "\n",
        "                            metrics[\"max_ccn\"] = max(ccn_list)\n",
        "                            metrics[\"avg_ccn\"] = round(np.mean(ccn_list), 2)\n",
        "                            metrics[\"tokens\"] = analysis.token_count\n",
        "                            metrics[\"avg_params\"] = round(np.mean(param_list), 2)\n",
        "                            metrics[\"loc\"] = analysis.nloc\n",
        "                        else:\n",
        "                            # Fallback for structural snippets lacking formal definitions\n",
        "                            metrics[\"max_ccn\"] = 1\n",
        "                            metrics[\"avg_ccn\"] = 1.0\n",
        "                            metrics[\"tokens\"] = analysis.token_count\n",
        "                            metrics[\"avg_params\"] = 0\n",
        "                            metrics[\"loc\"] = len(code.splitlines())\n",
        "\n",
        "                        stats[\"processed\"] += 1\n",
        "                    except Exception:\n",
        "                        stats[\"skipped\"] += 1\n",
        "\n",
        "                    entry[\"metrics_static\"] = metrics\n",
        "\n",
        "    print(f\"Complexity metrics updated. Processed: {stats['processed']} | Skipped: {stats['skipped']}\")\n",
        "\n",
        "# Execute the analysis on the global dataset\n",
        "calculate_complexity_expanded(dataset)\n",
        "\n",
        "# ==========================================\n",
        "# --- VALIDATION REPORT OUTPUT ---\n",
        "# ==========================================\n",
        "def verify_static_metrics_expanded(dataset):\n",
        "    print(\"\\nVALIDATION: EXPANDED STATIC METRICS (Spot-Check: 2 per Model)\")\n",
        "    print(\"-\" * 115)\n",
        "    records = []\n",
        "\n",
        "    for model in sorted(dataset.keys()):\n",
        "        model_count = 0\n",
        "        for topic in dataset[model]:\n",
        "            if model_count >= 2:\n",
        "                break\n",
        "            for entry in dataset[model][topic]:\n",
        "                if model_count < 2:\n",
        "                    m = entry.get(\"metrics_static\", {})\n",
        "                    records.append({\n",
        "                        \"Model\": model[:15],  # Expanded slightly for better readability\n",
        "                        \"Topic\": topic[:25],\n",
        "                        \"Iter\": entry.get(\"iteration\"),\n",
        "                        \"Max_CCN\": m.get(\"max_ccn\"),\n",
        "                        \"Avg_CCN\": m.get(\"avg_ccn\"),\n",
        "                        \"Tokens\": m.get(\"tokens\"),\n",
        "                        \"LOC\": m.get(\"loc\")\n",
        "                    })\n",
        "                    model_count += 1\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    print(df.to_string(index=False))\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "verify_static_metrics_expanded(dataset)"
      ],
      "metadata": {
        "id": "VMeUal0wOG-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Static Analysis: Error Density (cppcheck)\n",
        "This cell executes the second phase of the static audit. It leverages `cppcheck` (v.2.7-1) to identify high-severity logical flaws that bypass standard compilers (e.g., uninitialized variables, null pointer dereferences).\n",
        "\n",
        "To ensure fair comparison across models with varying verbosity, the raw error count is normalized against the file's Lines of Code (LOC) to generate the **Static Error Density (SED)** score. This metric is central to evaluating architectural efficiency (RQ2)."
      ],
      "metadata": {
        "id": "tCIiXt7VOHS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- ROBUST STATIC ERROR DENSITY (SED) ANALYSIS ---\n",
        "# ==========================================\n",
        "import subprocess\n",
        "import os\n",
        "import tempfile\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_sed_robust(dataset):\n",
        "    \"\"\"\n",
        "    Analyzes SED using cppcheck and normalizes counts against file-level LOC.\n",
        "    \"\"\"\n",
        "    print(\"Analyzing Static Error Density (SED) via cppcheck...\")\n",
        "    stats = {\"evaluated\": 0, \"failures\": 0}\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "        tmp_path = os.path.join(tmp_dir, \"sed_target.c\")\n",
        "\n",
        "        for model in dataset:\n",
        "            for topic in dataset[model]:\n",
        "                for entry in dataset[model][topic]:\n",
        "                    code = entry.get(\"extracted_code\", \"\")\n",
        "                    metrics = entry.get(\"metrics_static\", {})\n",
        "                    loc = metrics.get(\"loc\", 0)\n",
        "\n",
        "                    if not code or loc == 0:\n",
        "                        metrics[\"static_errors\"] = 0\n",
        "                        metrics[\"sed_score\"] = 0.0\n",
        "                        continue\n",
        "\n",
        "                    with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                        f.write(code)\n",
        "\n",
        "                    try:\n",
        "                        # XML version 2 provides structured data for precise error counting\n",
        "                        result = subprocess.run(\n",
        "                            [\"cppcheck\", \"--enable=all\", \"--xml\", \"--xml-version=2\", tmp_path],\n",
        "                            capture_output=True, text=True, check=False\n",
        "                        )\n",
        "\n",
        "                        error_count = result.stderr.count(\"<error \")\n",
        "                        metrics[\"static_errors\"] = error_count\n",
        "                        metrics[\"sed_score\"] = round(error_count / loc, 4) if loc > 0 else 0.0\n",
        "                        stats[\"evaluated\"] += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        metrics[\"static_errors\"] = -1\n",
        "                        metrics[\"sed_score\"] = -1.0\n",
        "                        stats[\"failures\"] += 1\n",
        "\n",
        "                    entry[\"metrics_static\"] = metrics\n",
        "\n",
        "    print(f\"SED analysis complete. Evaluated: {stats['evaluated']} | Failures: {stats['failures']}\")\n",
        "\n",
        "calculate_sed_robust(dataset)\n",
        "\n",
        "# ==========================================\n",
        "# --- VALIDATION REPORT OUTPUT ---\n",
        "# ==========================================\n",
        "def verify_sed_robust(dataset):\n",
        "    print(\"\\nVALIDATION: SED RE-CHECK (Spot-Check: 2 per Model)\")\n",
        "    print(\"-\" * 115)\n",
        "    records = []\n",
        "\n",
        "    for model in sorted(dataset.keys()):\n",
        "        model_count = 0\n",
        "        for topic in dataset[model]:\n",
        "            if model_count >= 2:\n",
        "                break\n",
        "            for entry in dataset[model][topic]:\n",
        "                if model_count < 2:\n",
        "                    m = entry.get(\"metrics_static\", {})\n",
        "                    records.append({\n",
        "                        \"Model\": model[:15],\n",
        "                        \"Topic\": topic[:25],\n",
        "                        \"Iter\": entry.get(\"iteration\"),\n",
        "                        \"Errors\": m.get(\"static_errors\"),\n",
        "                        \"SED_Score\": m.get(\"sed_score\"),\n",
        "                        \"LOC\": m.get(\"loc\")\n",
        "                    })\n",
        "                    model_count += 1\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    print(df.to_string(index=False))\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "verify_sed_robust(dataset)"
      ],
      "metadata": {
        "id": "oj59a0-EOUIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Static Analysis: SED Logic Verification\n",
        "To ensure the integrity of the Static Error Density (SED) metric, this validation cell performs a deep inspection of the `cppcheck` outputs.\n",
        "\n",
        "By isolating a flagged iteration and parsing the raw XML output, this script proves that the automated pipeline is successfully capturing genuine C memory and logic violations (e.g., `nullPointer`, `uninitvar`, `memleak`), rather than superficial environment or missing-path warnings."
      ],
      "metadata": {
        "id": "ZpOWv6dfOUep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- SED LOGIC VERIFICATION & DEBUGGING ---\n",
        "# ==========================================\n",
        "import os\n",
        "import tempfile\n",
        "import subprocess\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def verify_sed_error_content(dataset):\n",
        "    \"\"\"\n",
        "    Pulls the actual error message from the first flagged entry to confirm\n",
        "    we are catching high-severity C errors and not system/path errors.\n",
        "    \"\"\"\n",
        "    print(\"VALIDATION: SED ERROR TEXT EXTRACTION\")\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "    # Grab the first available entry that has an error > 0\n",
        "    sample_entry = None\n",
        "    for model in dataset:\n",
        "        for topic in dataset[model]:\n",
        "            for entry in dataset[model][topic]:\n",
        "                if entry.get(\"metrics_static\", {}).get(\"static_errors\", 0) > 0:\n",
        "                    sample_entry = entry\n",
        "                    break\n",
        "            if sample_entry:\n",
        "                break\n",
        "        if sample_entry:\n",
        "            break\n",
        "\n",
        "    if not sample_entry:\n",
        "        print(\"No static errors found in the dataset to verify.\")\n",
        "        print(\"-\" * 115)\n",
        "        return\n",
        "\n",
        "    # Extract data for the test\n",
        "    code = sample_entry.get(\"extracted_code\", \"\")\n",
        "    model_source = sample_entry.get(\"model\", \"Unknown\")\n",
        "    iter_id = sample_entry.get(\"iteration\", \"N/A\")\n",
        "\n",
        "    print(f\"Target Acquired: {model_source} (Iteration {iter_id})\")\n",
        "\n",
        "    # Safely write to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".c\", mode=\"w\", delete=False, encoding=\"utf-8\") as tmp:\n",
        "        tmp.write(code)\n",
        "        tmp_path = tmp.name\n",
        "\n",
        "    # Run cppcheck and capture the raw XML output\n",
        "    result = subprocess.run(\n",
        "        [\"cppcheck\", \"--enable=all\", \"--xml\", \"--xml-version=2\", tmp_path],\n",
        "        capture_output=True, text=True, check=False\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Raw XML Error Parsing ---\")\n",
        "    try:\n",
        "        # Parse XML to find the specific 'id' and 'msg' attributes\n",
        "        root = ET.fromstring(result.stderr)\n",
        "        errors = root.findall(\".//error\")\n",
        "\n",
        "        if not errors:\n",
        "            print(\"No <error> tags found in XML output.\")\n",
        "        else:\n",
        "            for err in errors[:3]: # Limit to first 3 errors to keep output clean\n",
        "                err_id = err.get('id', 'UNKNOWN_ID')\n",
        "                err_msg = err.get('msg', 'No message provided')\n",
        "                print(f\"ID: {err_id:<20} | MSG: {err_msg}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing XML: {e}\")\n",
        "        print(f\"Raw Stderr Snippet: {result.stderr[:200]}\")\n",
        "\n",
        "    # Clean up the temporary file\n",
        "    os.remove(tmp_path)\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "verify_sed_error_content(dataset)"
      ],
      "metadata": {
        "id": "P_w_sqqVOvBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Static Analysis: Filtered Error Density (cppcheck)\n",
        "Standard static analysis tools often inflate error counts when analyzing standalone code snippets due to system configuration warnings (e.g., `missingInclude` when standard C libraries are not explicitly linked in the working directory).\n",
        "\n",
        "To ensure the Static Error Density (SED) metric is a pure reflection of the model's logical competence, this cell re-evaluates the `cppcheck` output, programmatically parsing the XML to filter out environmental noise. This guarantees the final SED score strictly represents high-severity semantic and memory flaws."
      ],
      "metadata": {
        "id": "JaNlCcGWOvao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- FILTERED STATIC ERROR DENSITY (SED) ---\n",
        "# ==========================================\n",
        "import os\n",
        "import subprocess\n",
        "import tempfile\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def calculate_sed_filtered(dataset):\n",
        "    \"\"\"\n",
        "    Re-evaluates SED by filtering out system configuration warnings (missingInclude).\n",
        "    This ensures the score reflects true code-logic errors rather than environment limits.\n",
        "    \"\"\"\n",
        "    print(\"Re-analyzing SED (Filtering System Noise)...\")\n",
        "    stats = {\"evaluated\": 0, \"actual_errors\": 0}\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "        tmp_path = os.path.join(tmp_dir, \"sed_filter.c\")\n",
        "\n",
        "        for model in dataset:\n",
        "            for topic in dataset[model]:\n",
        "                for entry in dataset[model][topic]:\n",
        "                    code = entry.get(\"extracted_code\", \"\")\n",
        "                    metrics = entry.get(\"metrics_static\", {})\n",
        "                    loc = metrics.get(\"loc\", 0)\n",
        "\n",
        "                    if not code or loc == 0:\n",
        "                        continue\n",
        "\n",
        "                    # Safely write to temporary target\n",
        "                    with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                        f.write(code)\n",
        "\n",
        "                    try:\n",
        "                        # Use subprocess to get raw XML\n",
        "                        result = subprocess.run(\n",
        "                            [\"cppcheck\", \"--enable=all\", \"--xml\", \"--xml-version=2\", tmp_path],\n",
        "                            capture_output=True, text=True, check=False\n",
        "                        )\n",
        "\n",
        "                        # Parse XML to count true errors while excluding 'missingInclude'\n",
        "                        try:\n",
        "                            root = ET.fromstring(result.stderr)\n",
        "                            # Filter errors: Keep if ID is not a 'missing include' warning\n",
        "                            valid_errors = [\n",
        "                                err for err in root.findall(\".//error\")\n",
        "                                if \"missingInclude\" not in err.get(\"id\")\n",
        "                            ]\n",
        "\n",
        "                            error_count = len(valid_errors)\n",
        "                            metrics[\"static_errors\"] = error_count\n",
        "                            metrics[\"sed_score\"] = round(error_count / loc, 4)\n",
        "\n",
        "                            stats[\"evaluated\"] += 1\n",
        "                            stats[\"actual_errors\"] += error_count\n",
        "\n",
        "                        except Exception:\n",
        "                            # Fallback if XML is malformed\n",
        "                            metrics[\"static_errors\"] = 0\n",
        "                            metrics[\"sed_score\"] = 0.0\n",
        "\n",
        "                    except Exception:\n",
        "                        metrics[\"static_errors\"] = -1\n",
        "\n",
        "                    entry[\"metrics_static\"] = metrics\n",
        "\n",
        "    print(f\"Filtered SED complete. Evaluated: {stats['evaluated']} | True Errors Found: {stats['actual_errors']}\")\n",
        "\n",
        "# Execute the filtered audit\n",
        "calculate_sed_filtered(dataset)\n",
        "\n",
        "# ==========================================\n",
        "# --- VALIDATION REPORT OUTPUT ---\n",
        "# ==========================================\n",
        "def verify_sed_filtered(dataset):\n",
        "    print(\"\\nVALIDATION: TRUE SED PERFORMANCE (Spot-Check: 2 per Model)\")\n",
        "    print(\"-\" * 115)\n",
        "    records = []\n",
        "\n",
        "    for model in sorted(dataset.keys()):\n",
        "        m_count = 0\n",
        "        for topic in dataset[model]:\n",
        "            if m_count >= 2:\n",
        "                break\n",
        "            for entry in dataset[model][topic]:\n",
        "                if m_count < 2:\n",
        "                    m = entry.get(\"metrics_static\", {})\n",
        "                    records.append({\n",
        "                        \"Model\": model[:15],\n",
        "                        \"Topic\": topic[:25],\n",
        "                        \"Iter\": entry.get(\"iteration\"),\n",
        "                        \"True_Errors\": m.get(\"static_errors\"),\n",
        "                        \"SED_Score\": m.get(\"sed_score\")\n",
        "                    })\n",
        "                    m_count += 1\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    print(df.to_string(index=False))\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "verify_sed_filtered(dataset)"
      ],
      "metadata": {
        "id": "xu-X6MHQOwnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Static Analysis: Stylistic Adherence (clang-format)\n",
        "This cell concludes the static analysis phase by quantifying the stylistic cleanliness of the generated C solutions. It leverages `clang-format` configured to the industry-standard LLVM style guide.\n",
        "\n",
        "By counting the discrete `<replacement>` tags required to bring the code into full compliance, this metric generates the \"Style Deviations\" score. This data is critical for investigating the \"Style-Safety Paradox\", testing the hypothesis that LLMs may generate visually pristine code that simultaneously harbors catastrophic runtime flaws."
      ],
      "metadata": {
        "id": "xPtMjKcQOzQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- STYLE COMPLIANCE ANALYSIS (LLVM) ---\n",
        "# ==========================================\n",
        "import os\n",
        "import subprocess\n",
        "import tempfile\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_style_compliance(dataset):\n",
        "    \"\"\"\n",
        "    Utilizes clang-format to measure stylistic deviations.\n",
        "    Counts the number of discrete formatting fixes needed to match LLVM standards.\n",
        "    \"\"\"\n",
        "    print(\"Analyzing Style Compliance via clang-format...\")\n",
        "    stats = {\"processed\": 0, \"failures\": 0}\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "        tmp_path = os.path.join(tmp_dir, \"style_target.c\")\n",
        "\n",
        "        for model in dataset:\n",
        "            for topic in dataset[model]:\n",
        "                for entry in dataset[model][topic]:\n",
        "                    code = entry.get(\"extracted_code\", \"\")\n",
        "                    metrics = entry.get(\"metrics_static\", {})\n",
        "\n",
        "                    if not code:\n",
        "                        metrics[\"style_deviations\"] = 0\n",
        "                        continue\n",
        "\n",
        "                    # Safely write code to temporary file\n",
        "                    with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                        f.write(code)\n",
        "\n",
        "                    try:\n",
        "                        # --output-replacements-xml: Lists every required change in XML format\n",
        "                        # --style=LLVM: The industry standard for C code formatting\n",
        "                        style_proc = subprocess.run(\n",
        "                            [\"clang-format\", \"--output-replacements-xml\", \"--style=LLVM\", tmp_path],\n",
        "                            capture_output=True, text=True, check=False\n",
        "                        )\n",
        "\n",
        "                        # Each <replacement> tag represents a specific formatting deviation\n",
        "                        deviations = style_proc.stdout.count(\"<replacement \")\n",
        "                        metrics[\"style_deviations\"] = deviations\n",
        "                        stats[\"processed\"] += 1\n",
        "\n",
        "                    except Exception:\n",
        "                        metrics[\"style_deviations\"] = -1\n",
        "                        stats[\"failures\"] += 1\n",
        "\n",
        "                    entry[\"metrics_static\"] = metrics\n",
        "\n",
        "    print(f\"Style analysis complete. Processed: {stats['processed']} | Failures: {stats['failures']}\")\n",
        "\n",
        "# Execute Style Analysis\n",
        "calculate_style_compliance(dataset)\n",
        "\n",
        "# ==========================================\n",
        "# --- VALIDATION REPORT OUTPUT ---\n",
        "# ==========================================\n",
        "def verify_style_results(dataset):\n",
        "    print(\"\\nVALIDATION: STYLE COMPLIANCE SAMPLE (Spot-Check: 2 per Model)\")\n",
        "    print(\"-\" * 115)\n",
        "    records = []\n",
        "\n",
        "    for model in sorted(dataset.keys()):\n",
        "        model_count = 0\n",
        "        for topic in dataset[model]:\n",
        "            if model_count >= 2:\n",
        "                break\n",
        "            for entry in dataset[model][topic]:\n",
        "                if model_count < 2:\n",
        "                    m = entry.get(\"metrics_static\", {})\n",
        "                    records.append({\n",
        "                        \"Model\": model[:15],\n",
        "                        \"Topic\": topic[:25],\n",
        "                        \"Iter\": entry.get(\"iteration\"),\n",
        "                        \"Style_Dev\": m.get(\"style_deviations\"),\n",
        "                        \"LOC\": m.get(\"loc\")\n",
        "                    })\n",
        "                    model_count += 1\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    print(df.to_string(index=False))\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "verify_style_results(dataset)"
      ],
      "metadata": {
        "id": "hHlz_QP8PEyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Dynamic Analysis: Strict Compilation (CSR)\n",
        "This cell initiates the dynamic runtime audit. Each generated solution is subjected to strict compilation using the `gcc` compiler (v.11.4.0).\n",
        "\n",
        "To ensure the code meets rigorous academic standards, the compilation strictly enforces the C11 standard and elevates all warnings to fatal errors using the `-Wall -Werror -std=c11` flags. The resulting **Compilation Success Rate (CSR)** serves as the baseline metric for syntactic fluency, separating structurally viable code from catastrophic generation failures before memory instrumentation begins. Successfully compiled binaries are persisted to the Drive for the subsequent Valgrind/UBSan audits."
      ],
      "metadata": {
        "id": "psIQEz3YPGmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- STRICT COMPILATION & BINARY PERSISTENCE ---\n",
        "# ==========================================\n",
        "import os\n",
        "import subprocess\n",
        "import tempfile\n",
        "import pandas as pd\n",
        "\n",
        "# Setup Binary Storage Directory (Uses the verified path from Cell 1)\n",
        "BIN_DIR = os.path.join(path, \"binaries\")\n",
        "if not os.path.exists(BIN_DIR):\n",
        "    os.makedirs(BIN_DIR)\n",
        "    print(f\"Created binary storage at: {BIN_DIR}\")\n",
        "\n",
        "def run_compilation_audit(dataset):\n",
        "    \"\"\"\n",
        "    Attempts to compile C code with strict flags: -Wall -Werror -std=c11.\n",
        "    Successful binaries and error logs are stored on Drive.\n",
        "    \"\"\"\n",
        "    print(\"Starting Strict Compilation Audit (Calculating CSR)...\")\n",
        "    stats = {\"pass\": 0, \"fail\": 0}\n",
        "\n",
        "    for model in dataset:\n",
        "        # Create model-specific subfolders for clean binary organization\n",
        "        model_safe = model.replace(\"/\", \"_\")\n",
        "        model_path = os.path.join(BIN_DIR, model_safe)\n",
        "        if not os.path.exists(model_path):\n",
        "            os.makedirs(model_path)\n",
        "\n",
        "        for topic in dataset[model]:\n",
        "            for entry in dataset[model][topic]:\n",
        "                code = entry.get(\"extracted_code\", \"\")\n",
        "                it_id = entry.get(\"iteration\", \"unknown\")\n",
        "\n",
        "                # Initialize dynamic metrics dictionary\n",
        "                dyn_metrics = entry.get(\"metrics_dynamic\", {})\n",
        "\n",
        "                if not code:\n",
        "                    dyn_metrics[\"compilation_status\"] = \"FAIL_EMPTY\"\n",
        "                    dyn_metrics[\"csr\"] = 0\n",
        "                    entry[\"metrics_dynamic\"] = dyn_metrics\n",
        "                    stats[\"fail\"] += 1\n",
        "                    continue\n",
        "\n",
        "                # Define persistent paths for the binary and potential error logs\n",
        "                bin_file = os.path.join(model_path, f\"it_{it_id}.out\")\n",
        "                log_file = os.path.join(model_path, f\"it_{it_id}.log\")\n",
        "\n",
        "                # Write code to a temporary .c file for GCC to target\n",
        "                with tempfile.NamedTemporaryFile(suffix=\".c\", mode=\"w\", delete=False, encoding=\"utf-8\") as tmp:\n",
        "                    tmp.write(code)\n",
        "                    tmp_c = tmp.name\n",
        "\n",
        "                try:\n",
        "                    # Execute strict compilation command per Section 3.4 methodology\n",
        "                    comp_proc = subprocess.run(\n",
        "                        [\"gcc\", \"-Wall\", \"-Werror\", \"-std=c11\", tmp_c, \"-o\", bin_file],\n",
        "                        capture_output=True, text=True, check=False\n",
        "                    )\n",
        "\n",
        "                    if comp_proc.returncode == 0:\n",
        "                        dyn_metrics[\"compilation_status\"] = \"PASS\"\n",
        "                        dyn_metrics[\"csr\"] = 1\n",
        "                        entry[\"binary_path\"] = bin_file # Store persistent path for Valgrind\n",
        "                        stats[\"pass\"] += 1\n",
        "                    else:\n",
        "                        dyn_metrics[\"compilation_status\"] = \"FAIL_GCC\"\n",
        "                        dyn_metrics[\"csr\"] = 0\n",
        "                        # Save the GCC error log for qualitative 'Oracle Hazard' analysis\n",
        "                        with open(log_file, \"w\", encoding=\"utf-8\") as log:\n",
        "                            log.write(comp_proc.stderr)\n",
        "                        stats[\"fail\"] += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    dyn_metrics[\"compilation_status\"] = \"SYSTEM_ERROR\"\n",
        "                    dyn_metrics[\"csr\"] = 0\n",
        "                    stats[\"fail\"] += 1\n",
        "\n",
        "                finally:\n",
        "                    # Always clean up the temporary .c file\n",
        "                    if os.path.exists(tmp_c):\n",
        "                        os.remove(tmp_c)\n",
        "\n",
        "                entry[\"metrics_dynamic\"] = dyn_metrics\n",
        "\n",
        "    print(f\"Compilation Audit Complete. PASS: {stats['pass']} | FAIL: {stats['fail']}\")\n",
        "\n",
        "# Execute the GCC compilation audit\n",
        "run_compilation_audit(dataset)\n",
        "\n",
        "# ==========================================\n",
        "# --- VALIDATION REPORT OUTPUT ---\n",
        "# ==========================================\n",
        "def verify_compilation_results(dataset):\n",
        "    print(\"\\nVALIDATION: COMPILATION SUCCESS SAMPLE (Spot-Check: 2 per Model)\")\n",
        "    print(\"-\" * 115)\n",
        "    records = []\n",
        "\n",
        "    for model in sorted(dataset.keys()):\n",
        "        m_count = 0\n",
        "        for topic in dataset[model]:\n",
        "            if m_count >= 2:\n",
        "                break\n",
        "            for entry in dataset[model][topic]:\n",
        "                if m_count < 2:\n",
        "                    d = entry.get(\"metrics_dynamic\", {})\n",
        "                    records.append({\n",
        "                        \"Model\": model[:15],\n",
        "                        \"Topic\": topic[:25],\n",
        "                        \"Iter\": entry.get(\"iteration\"),\n",
        "                        \"Status\": d.get(\"compilation_status\"),\n",
        "                        \"CSR_Binary\": d.get(\"csr\", 0)\n",
        "                    })\n",
        "                    m_count += 1\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    print(df.to_string(index=False))\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "verify_compilation_results(dataset)"
      ],
      "metadata": {
        "id": "tV8XCmFDPYa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dynamic Analysis: Test Suite Extraction\n",
        "To fully automate the dynamic analysis, the pipeline must feed the generated C binaries with valid input data.\n",
        "\n",
        "As outlined in the multi-turn generation protocol, Step 6 tasked the LLM with generating a machine-readable JSON block containing comprehensive test cases. This cell parses that output, extracting the required \"Handshake\" input string and the specific `exit_command` needed to allow the program to terminate safely. These extracted drivers will be used to instrument the Valgrind and execution environments in the subsequent cells."
      ],
      "metadata": {
        "id": "K2VCwQabPY7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- TEST METADATA EXTRACTION (STEP 6) ---\n",
        "# ==========================================\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def extract_test_metadata(dataset):\n",
        "    \"\"\"\n",
        "    Parses the JSON blocks in Step 6 to extract automated test drivers.\n",
        "    Metadata includes the exit command and the primary test input string.\n",
        "    \"\"\"\n",
        "    print(\"Extracting Test Drivers from Step 6 Generation...\")\n",
        "    stats = {\"found\": 0, \"missing\": 0}\n",
        "\n",
        "    for model in dataset:\n",
        "        for topic in dataset[model]:\n",
        "            for entry in dataset[model][topic]:\n",
        "                # Locate Step 6 text\n",
        "                step6_text = entry.get('steps', {}).get('step_6', \"\")\n",
        "\n",
        "                # Regex to isolate the JSON block within the Step 6 prose\n",
        "                json_match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", step6_text, re.DOTALL)\n",
        "\n",
        "                if json_match:\n",
        "                    try:\n",
        "                        test_json = json.loads(json_match.group(1))\n",
        "\n",
        "                        # Safely extract the first input string to avoid IndexError on empty arrays\n",
        "                        test_suite = test_json.get(\"test_suite\", [])\n",
        "                        first_input = test_suite[0].get(\"input\", \"\") if len(test_suite) > 0 else \"\"\n",
        "\n",
        "                        # Capture the exit command and the test input\n",
        "                        entry[\"test_metadata\"] = {\n",
        "                            \"exit_cmd\": str(test_json.get(\"exit_command\", \"0\")),\n",
        "                            \"input_str\": first_input\n",
        "                        }\n",
        "                        stats[\"found\"] += 1\n",
        "\n",
        "                    except Exception:\n",
        "                        entry[\"test_metadata\"] = None\n",
        "                        stats[\"missing\"] += 1\n",
        "                else:\n",
        "                    entry[\"test_metadata\"] = None\n",
        "                    stats[\"missing\"] += 1\n",
        "\n",
        "    print(f\"Metadata Extraction Complete. Drivers Found: {stats['found']} | Missing/Malformed: {stats['missing']}\")\n",
        "\n",
        "# Execute Extraction\n",
        "extract_test_metadata(dataset)\n",
        "\n",
        "# ==========================================\n",
        "# --- VALIDATION REPORT OUTPUT ---\n",
        "# ==========================================\n",
        "def verify_test_metadata(dataset):\n",
        "    print(\"\\nVALIDATION: TEST DRIVER SAMPLE (Spot-Check: 1 per Model)\")\n",
        "    print(\"-\" * 115)\n",
        "    for model in sorted(dataset.keys()):\n",
        "        for topic in dataset[model]:\n",
        "            if len(dataset[model][topic]) == 0:\n",
        "                continue\n",
        "\n",
        "            entry = dataset[model][topic][0]\n",
        "            meta = entry.get(\"test_metadata\")\n",
        "\n",
        "            if meta:\n",
        "                # Replace actual newlines with literal '\\n' characters for compact terminal display\n",
        "                input_preview = meta['input_str'].replace('\\n', '\\\\n')\n",
        "                print(f\"{model[:15]:<15} | Exit: {meta['exit_cmd']:<3} | Input: {input_preview[:50]}...\")\n",
        "                break # Only show one per model for the preview\n",
        "\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "verify_test_metadata(dataset)"
      ],
      "metadata": {
        "id": "09kTGhArQCKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1. Dynamic Analysis: Robust Metadata & Safety Injection\n",
        "Given the probabilistic nature of LLMs, the generated JSON test suites in Step 6 occasionally contain malformed syntax or missing `exit_command` fields.\n",
        "\n",
        "To ensure maximum dataset retention, this cell implements a robust fallback parser. More importantly, it appends a **\"Safety Sequence\"** (a cascade of common exit integers and string commands) to the extracted test drivers. This mitigates catastrophic \"logical hangs (infinite loops)\" during the dynamic execution phase. By forcing a stuck program to exit naturally rather than requiring a hard system-level `SIGKILL`, the pipeline ensures that the Valgrind memory profiler can successfully generate its leak report."
      ],
      "metadata": {
        "id": "4lk2AAqCQCm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- ROBUST TEST METADATA EXTRACTION ---\n",
        "# ==========================================\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def extract_test_metadata_robust(dataset):\n",
        "    \"\"\"\n",
        "    Parses Step 6 for JSON test drivers using aggressive regex matching.\n",
        "    Implements a fallback 'Safety Sequence' to mitigate logical hangs\n",
        "    during dynamic analysis if the model-provided exit command is incorrect.\n",
        "    \"\"\"\n",
        "    print(\"Extracting Robust Test Drivers from Step 6...\")\n",
        "    stats = {\"found\": 0, \"missing\": 0, \"fixed_exit\": 0}\n",
        "\n",
        "    for model in dataset:\n",
        "        for topic in dataset[model]:\n",
        "            for entry in dataset[model][topic]:\n",
        "                step6_text = entry.get('steps', {}).get('step_6', \"\")\n",
        "\n",
        "                # Broaden regex to find JSON even if the ```json markdown tag is missing\n",
        "                json_match = re.search(r\"(\\{.*\\})\", step6_text.replace('\\n', ' '), re.DOTALL)\n",
        "\n",
        "                meta_found = False\n",
        "                if json_match:\n",
        "                    try:\n",
        "                        # Clean the match to ensure it only contains the JSON object\n",
        "                        clean_json = json_match.group(1)\n",
        "                        test_json = json.loads(clean_json)\n",
        "\n",
        "                        exit_cmd = str(test_json.get(\"exit_command\", \"\")).strip()\n",
        "                        input_str = \"\"\n",
        "\n",
        "                        # Extract first input from the test suite securely\n",
        "                        if \"test_suite\" in test_json and len(test_json[\"test_suite\"]) > 0:\n",
        "                            input_str = test_json[\"test_suite\"][0].get(\"input\", \"\")\n",
        "\n",
        "                        # Validation: If exit_cmd is missing, attempt to infer it or set a flag\n",
        "                        if not exit_cmd or exit_cmd == \"\":\n",
        "                            exit_cmd = \"0\" # Default fallback\n",
        "                            stats[\"fixed_exit\"] += 1\n",
        "\n",
        "                        entry[\"test_metadata\"] = {\n",
        "                            \"exit_cmd\": exit_cmd,\n",
        "                            \"input_str\": input_str,\n",
        "                            # A 'Safety Sequence' appended to execution buffers to prevent hard hangs\n",
        "                            \"safety_exit_seq\": \"\\n0\\n4\\n5\\n9\\nexit\\nquit\\n\"\n",
        "                        }\n",
        "                        meta_found = True\n",
        "                    except Exception:\n",
        "                        pass # Fall through to the missing counter if parsing completely fails\n",
        "\n",
        "                if meta_found:\n",
        "                    stats[\"found\"] += 1\n",
        "                else:\n",
        "                    entry[\"test_metadata\"] = None\n",
        "                    stats[\"missing\"] += 1\n",
        "\n",
        "    print(f\"Robust Extraction Complete.\")\n",
        "    print(f\"Drivers Found: {stats['found']} | Missing: {stats['missing']} | Defaulted Exits: {stats['fixed_exit']}\")\n",
        "\n",
        "# Execute Robust Extraction\n",
        "extract_test_metadata_robust(dataset)\n",
        "\n",
        "# ==========================================\n",
        "# --- VALIDATION REPORT OUTPUT ---\n",
        "# ==========================================\n",
        "def verify_metadata_alignment(dataset):\n",
        "    \"\"\"\n",
        "    Verifies that the extracted input strings match the expected topic content.\n",
        "    \"\"\"\n",
        "    print(\"\\nVALIDATION: METADATA ALIGNMENT CHECK (1 per Topic per Model)\")\n",
        "    print(\"-\" * 115)\n",
        "    header = f\"{'MODEL':<15} | {'TOPIC':<25} | {'ITER':<5} | {'EXIT':<5} | {'INPUT PREVIEW'}\"\n",
        "    print(header)\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "    for model in sorted(dataset.keys()):\n",
        "        for topic in sorted(dataset[model].keys()):\n",
        "            if len(dataset[model][topic]) == 0:\n",
        "                continue\n",
        "\n",
        "            entry = dataset[model][topic][0]\n",
        "            meta = entry.get(\"test_metadata\")\n",
        "\n",
        "            if meta:\n",
        "                input_prev = meta['input_str'].replace('\\n', '\\\\n')[:45]\n",
        "                print(f\"{model[:15]:<15} | {topic[:25]:<25} | {entry.get('iteration'):<5} | {meta['exit_cmd']:<5} | {input_prev}...\")\n",
        "\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "verify_metadata_alignment(dataset)"
      ],
      "metadata": {
        "id": "cU_TnsTzQS5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Dynamic Analysis: Valgrind Environment Handshake\n",
        "Before executing the full-scale memory audit on the dataset, this cell performs a single-sample \"smoke test\".\n",
        "\n",
        "In other words, it verifies that the dynamic analysis environment is correctly configured with `Valgrind` (v.3.18.1+) and confirms that the compiled C binaries can successfully interface with the automated input drivers extracted in Step 6. This handshake ensures that the pipeline can accurately capture \"Definitely Lost\" heap memory and invalid memory access violations without succumbing to system-level hangs."
      ],
      "metadata": {
        "id": "SlqVbKvYQT0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- SINGLE-SAMPLE VALGRIND HANDSHAKE ---\n",
        "# ==========================================\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "def run_valgrind_handshake(dataset):\n",
        "    \"\"\"\n",
        "    Executes a single-sample test to verify the dynamic analysis environment.\n",
        "    Confirmed compatibility between binaries and extracted test drivers is required\n",
        "    before the full-scale audit.\n",
        "    \"\"\"\n",
        "    print(\"Starting Valgrind Handshake (Single-Sample Verification)...\")\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "    # 1. Identify a suitable sample for testing\n",
        "    target_entry = None\n",
        "    target_model = None\n",
        "\n",
        "    for model in dataset:\n",
        "        for topic in dataset[model]:\n",
        "            for entry in dataset[model][topic]:\n",
        "                # Sample must have a persistent binary and test metadata\n",
        "                if entry.get(\"metrics_dynamic\", {}).get(\"csr\") == 1 and entry.get(\"test_metadata\"):\n",
        "                    target_entry = entry\n",
        "                    target_model = model\n",
        "                    break\n",
        "            if target_entry:\n",
        "                break\n",
        "        if target_entry:\n",
        "            break\n",
        "\n",
        "    if not target_entry:\n",
        "        print(\"Critical Failure: No compiled binaries with metadata found.\")\n",
        "        print(\"-\" * 115)\n",
        "        return\n",
        "\n",
        "    # 2. Prepare execution parameters\n",
        "    bin_path = target_entry.get(\"binary_path\")\n",
        "    meta = target_entry[\"test_metadata\"]\n",
        "\n",
        "    # Construct input stream: Test Input + Safety Exit Sequence\n",
        "    # This prevents the process from hanging if the primary exit command fails\n",
        "    full_input = f\"{meta['input_str']}\\n{meta['exit_cmd']}{meta['safety_exit_seq']}\"\n",
        "\n",
        "    print(f\"Testing Model: {target_model}\")\n",
        "    print(f\"Target Binary: {bin_path}\")\n",
        "\n",
        "    try:\n",
        "        # 3. Execute Valgrind\n",
        "        # --leak-check=full: Tracks every heap allocation\n",
        "        # --errors-for-leak-kinds=all: Ensures all leak types are reported\n",
        "        vg_proc = subprocess.run(\n",
        "            [\"valgrind\", \"--leak-check=full\", \"--errors-for-leak-kinds=all\", bin_path],\n",
        "            input=full_input,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            encoding=\"utf-8\",\n",
        "            timeout=15, # Extended timeout for Valgrind overhead\n",
        "            errors='replace'\n",
        "        )\n",
        "\n",
        "        # 4. Analyze Output Log\n",
        "        output_log = vg_proc.stderr + vg_proc.stdout\n",
        "\n",
        "        # Logic: Search for indicators of memory safety\n",
        "        # 'definitely lost: 0' and 'ERROR SUMMARY: 0' indicate a clean execution\n",
        "        has_leaks = \"definitely lost:\" in output_log and \"definitely lost: 0\" not in output_log\n",
        "        has_errors = \"ERROR SUMMARY:\" in output_log and \"ERROR SUMMARY: 0\" not in output_log\n",
        "\n",
        "        print(\"\\n--- Valgrind Analysis Result ---\")\n",
        "        if not has_leaks and not has_errors:\n",
        "            print(\"Status: CLEAN (No leaks or invalid operations detected)\")\n",
        "        else:\n",
        "            print(\"Status: SAFETY_VIOLATION detected\")\n",
        "            if has_leaks:\n",
        "                print(\"   - Heap Leak Identified ('Definitely Lost' > 0)\")\n",
        "            if has_errors:\n",
        "                print(\"   - Invalid Memory Access Identified (e.g., Segfault/Invalid Read)\")\n",
        "\n",
        "        # 5. Validation Check: Termination\n",
        "        if vg_proc.returncode == 0:\n",
        "            print(\"Handshake Success: Program terminated gracefully.\")\n",
        "        else:\n",
        "            print(f\"Handshake Warning: Program exited with non-zero code ({vg_proc.returncode}).\")\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"Handshake Failure: Program timed out (Logical Hang).\")\n",
        "    except Exception as e:\n",
        "        print(f\"System Error during handshake: {e}\")\n",
        "\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "\n",
        "# Install Valgrind if not present (Verification step matching methodology v.3.18.1+)\n",
        "print(\"Verifying Valgrind Installation...\")\n",
        "!apt-get install -y valgrind -qq\n",
        "\n",
        "# Execute Handshake\n",
        "run_valgrind_handshake(dataset)"
      ],
      "metadata": {
        "id": "glfeqAmQQ1CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Dynamic Analysis: Full-Scale Memory & Runtime Audit (Valgrind)\n",
        "This cell executes the core dynamic evaluation of the 1195-iteration dataset. Every successfully compiled binary is subjected to execution under `Valgrind`, fed with the automated test drivers extracted in Step 6.\n",
        "\n",
        "**Key Metrics Extracted (Section 4):**\n",
        "* **Memory Safety Rate:** The percentage of executed programs that successfully terminate without triggering \"Definitely Lost\" heap memory leaks or invalid access violations.\n",
        "* **Logical Hangs:** The incidence of models trapping the execution thread in infinite loops (caught via a strict 5-second execution timeout).\n",
        "\n",
        "*Note: Due to the high computational overhead of memory instrumentation, this process may take several minutes. A progress bar is provided to track real-time execution.*"
      ],
      "metadata": {
        "id": "76rmv6ByQ1hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- FULL-SCALE VALGRIND HYBRID PIPELINE ---\n",
        "# ==========================================\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "def run_full_valgrind_audit_with_progress(dataset):\n",
        "    \"\"\"\n",
        "    Performs the memory safety audit with real-time progress tracking.\n",
        "    Evaluates leaks, invalid memory accesses, and logical hangs.\n",
        "    \"\"\"\n",
        "    # 1. Flatten the entries to calculate the total execution batch\n",
        "    all_binaries = []\n",
        "    for model in dataset:\n",
        "        for topic in dataset[model]:\n",
        "            for entry in dataset[model][topic]:\n",
        "                if entry.get(\"metrics_dynamic\", {}).get(\"csr\") == 1 and entry.get(\"test_metadata\"):\n",
        "                    all_binaries.append(entry)\n",
        "\n",
        "    total_to_run = len(all_binaries)\n",
        "    print(\"Starting Full Dynamic Analysis (Valgrind)...\")\n",
        "    print(f\"Target: {total_to_run} compiled binaries detected.\")\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "    stats = {\"clean\": 0, \"safety_violation\": 0, \"timeout\": 0, \"error\": 0, \"processed\": 0}\n",
        "\n",
        "    # 2. Initialize the dynamic Progress Bar\n",
        "    progress_bar = tqdm(total=total_to_run, desc=\"Auditing Binaries\", unit=\"bin\")\n",
        "\n",
        "    for entry in all_binaries:\n",
        "        dyn = entry.get(\"metrics_dynamic\")\n",
        "        bin_path = entry.get(\"binary_path\")\n",
        "        meta = entry.get(\"test_metadata\")\n",
        "\n",
        "        # Prepare the input stream (User Input + Safety Exit Sequence)\n",
        "        full_input = f\"{meta['input_str']}\\n{meta['exit_cmd']}{meta['safety_exit_seq']}\"\n",
        "\n",
        "        try:\n",
        "            # Execute Valgrind with strict 5-second timeout to catch infinite loops\n",
        "            vg_proc = subprocess.run(\n",
        "                [\"valgrind\", \"--leak-check=full\", \"--errors-for-leak-kinds=all\", bin_path],\n",
        "                input=full_input,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                encoding=\"utf-8\",\n",
        "                timeout=5,\n",
        "                errors='replace'\n",
        "            )\n",
        "\n",
        "            output_log = vg_proc.stderr + vg_proc.stdout\n",
        "\n",
        "            # Logic: Identify \"Definitely Lost\" heap leaks or invalid accesses\n",
        "            has_leaks = \"definitely lost:\" in output_log and \"definitely lost: 0\" not in output_log\n",
        "            has_errors = \"ERROR SUMMARY:\" in output_log and \"ERROR SUMMARY: 0\" not in output_log\n",
        "\n",
        "            if not has_leaks and not has_errors:\n",
        "                dyn[\"valgrind_status\"] = \"CLEAN\"\n",
        "                dyn[\"mem_safety_score\"] = 1\n",
        "                stats[\"clean\"] += 1\n",
        "            else:\n",
        "                dyn[\"valgrind_status\"] = \"SAFETY_VIOLATION\"\n",
        "                dyn[\"mem_safety_score\"] = 0\n",
        "                stats[\"safety_violation\"] += 1\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            # Captures 'Logical Hangs' as defined in the methodology\n",
        "            dyn[\"valgrind_status\"] = \"TIMEOUT_LOGIC_HANG\"\n",
        "            dyn[\"mem_safety_score\"] = 0\n",
        "            stats[\"timeout\"] += 1\n",
        "\n",
        "        except Exception:\n",
        "            dyn[\"valgrind_status\"] = \"EXECUTION_ERROR\"\n",
        "            dyn[\"mem_safety_score\"] = 0\n",
        "            stats[\"error\"] += 1\n",
        "\n",
        "        entry[\"metrics_dynamic\"] = dyn\n",
        "\n",
        "        # 3. Update Progress Bar state\n",
        "        stats[\"processed\"] += 1\n",
        "        progress_bar.update(1)\n",
        "\n",
        "        # Periodic Checkpoint Print (Updates the text next to the progress bar)\n",
        "        if stats[\"processed\"] % 10 == 0:\n",
        "            progress_bar.set_postfix(clean=stats[\"clean\"], fails=stats[\"safety_violation\"], hangs=stats[\"timeout\"])\n",
        "\n",
        "    progress_bar.close()\n",
        "\n",
        "    # 4. Final Summary Report\n",
        "    print(\"\\n\" + \"=\" * 55)\n",
        "    print(\"FULL DYNAMIC ANALYSIS COMPLETE\")\n",
        "    print(\"=\" * 55)\n",
        "    print(f\"Clean Executions:      {stats['clean']}\")\n",
        "    print(f\"Safety Violations:     {stats['safety_violation']} (Leaks/UB)\")\n",
        "    print(f\"Logical Hangs:         {stats['timeout']} (Infinite Loops)\")\n",
        "    print(f\"System Errors:         {stats['error']}\")\n",
        "    print(\"-\" * 55)\n",
        "    print(f\"Total Binaries Tested: {stats['processed']} / {total_to_run}\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "# Execute the audit\n",
        "run_full_valgrind_audit_with_progress(dataset)"
      ],
      "metadata": {
        "id": "u16xIf2_RGCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Dynamic Analysis: Undefined Behavior Sanitizer (UBSan)\n",
        "This cell completes the dynamic analysis suite by evaluating the dataset against the UndefinedBehaviorSanitizer (UBSan).\n",
        "\n",
        "Standard testing and even Valgrind may overlook specific runtime illegalities (e.g., signed integer overflows, division by zero). To catch these \"Oracle Hazards\", this pipeline recompiles each successful C solution using `gcc` with the `-fsanitize=undefined` flag. The binaries are then executed, and any resulting `runtime error` logs are parsed to identify and penalize logical flaws."
      ],
      "metadata": {
        "id": "TEF-xWdERGY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- UNDEFINED BEHAVIOR (UBSAN) AUDIT ---\n",
        "# ==========================================\n",
        "import os\n",
        "import subprocess\n",
        "import tempfile\n",
        "import pandas as pd\n",
        "\n",
        "def run_ubsan_audit(dataset):\n",
        "    \"\"\"\n",
        "    Re-compiles and executes binaries with UndefinedBehaviorSanitizer.\n",
        "    Identifies silent logic failures like integer overflows and null-pointer usage.\n",
        "    \"\"\"\n",
        "    print(f\"Starting Undefined Behavior Audit (UBSan)...\")\n",
        "    stats = {\"clean\": 0, \"ub_detected\": 0, \"timeout\": 0, \"processed\": 0}\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "        for model in dataset:\n",
        "            for topic in dataset[model]:\n",
        "                for entry in dataset[model][topic]:\n",
        "                    # Only test binaries that passed the initial compilation\n",
        "                    if entry.get(\"metrics_dynamic\", {}).get(\"csr\") != 1 or not entry.get(\"test_metadata\"):\n",
        "                        continue\n",
        "\n",
        "                    code = entry.get(\"extracted_code\", \"\")\n",
        "                    meta = entry.get(\"test_metadata\")\n",
        "                    dyn = entry.get(\"metrics_dynamic\")\n",
        "\n",
        "                    # Compile with UBSan flags\n",
        "                    c_file = os.path.join(tmp_dir, \"ub_test.c\")\n",
        "                    bin_path = os.path.join(tmp_dir, \"ub_test.out\")\n",
        "                    with open(c_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                        f.write(code)\n",
        "\n",
        "                    try:\n",
        "                        # Re-compile strictly for UBSan instrumentation\n",
        "                        comp = subprocess.run(\n",
        "                            [\"gcc\", \"-fsanitize=undefined\", \"-g\", c_file, \"-o\", bin_path],\n",
        "                            capture_output=True, text=True, check=False, encoding=\"utf-8\"\n",
        "                        )\n",
        "\n",
        "                        if comp.returncode == 0:\n",
        "                            full_input = f\"{meta['input_str']}\\n{meta['exit_cmd']}{meta['safety_exit_seq']}\"\n",
        "\n",
        "                            # Execute the instrumented binary\n",
        "                            run = subprocess.run(\n",
        "                                [bin_path], input=full_input,\n",
        "                                capture_output=True, text=True, timeout=5, encoding=\"utf-8\", errors='replace'\n",
        "                            )\n",
        "\n",
        "                            # UBSan outputs errors to stderr starting with 'runtime error:'\n",
        "                            if \"runtime error:\" in run.stderr.lower():\n",
        "                                dyn[\"ub_status\"] = \"UB_TRIGGERED\"\n",
        "                                # Capture the specific error detail for analysis\n",
        "                                dyn[\"ub_detail\"] = run.stderr.split('\\n')[0]\n",
        "                                stats[\"ub_detected\"] += 1\n",
        "                            else:\n",
        "                                dyn[\"ub_status\"] = \"CLEAN\"\n",
        "                                stats[\"clean\"] += 1\n",
        "\n",
        "                            stats[\"processed\"] += 1\n",
        "\n",
        "                    except subprocess.TimeoutExpired:\n",
        "                        dyn[\"ub_status\"] = \"TIMEOUT\"\n",
        "                        stats[\"timeout\"] += 1\n",
        "                    except Exception:\n",
        "                        dyn[\"ub_status\"] = \"EXEC_ERR\"\n",
        "\n",
        "                    entry[\"metrics_dynamic\"] = dyn\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 55)\n",
        "    print(\"UBSAN ANALYSIS COMPLETE\")\n",
        "    print(\"=\" * 55)\n",
        "    print(f\"Clean (No UB):         {stats['clean']}\")\n",
        "    print(f\"UB Triggers Found:     {stats['ub_detected']}\")\n",
        "    print(f\"Logical Hangs:         {stats['timeout']}\")\n",
        "    print(\"-\" * 55)\n",
        "    print(f\"Total Binaries Tested: {stats['processed']}\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "# Execute UBSan Audit\n",
        "run_ubsan_audit(dataset)\n",
        "\n",
        "# ==========================================\n",
        "# --- VALIDATION REPORT OUTPUT ---\n",
        "# ==========================================\n",
        "def verify_ub_results(dataset):\n",
        "    print(\"\\nVALIDATION: UBSAN PERFORMANCE SAMPLE (Spot-Check: 2 per Model)\")\n",
        "    print(\"-\" * 115)\n",
        "    records = []\n",
        "\n",
        "    for model in sorted(dataset.keys()):\n",
        "        m_count = 0\n",
        "        for topic in dataset[model]:\n",
        "            if m_count >= 2:\n",
        "                break\n",
        "            for entry in dataset[model][topic]:\n",
        "                if m_count < 2 and \"ub_status\" in entry.get(\"metrics_dynamic\", {}):\n",
        "                    d = entry.get(\"metrics_dynamic\", {})\n",
        "                    records.append({\n",
        "                        \"Model\": model[:15],\n",
        "                        \"Topic\": topic[:20],\n",
        "                        \"Iter\": entry.get(\"iteration\"),\n",
        "                        \"UB_Status\": d.get(\"ub_status\"),\n",
        "                        \"UB_Detail\": (d.get(\"ub_detail\", \"\")[:40] + \"...\") if d.get(\"ub_detail\") else \"N/A\"\n",
        "                    })\n",
        "                    m_count += 1\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    print(df.to_string(index=False))\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "verify_ub_results(dataset)"
      ],
      "metadata": {
        "id": "_xQc4NBART3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Final Artifact: Master Dataset Consolidation\n",
        "This final cell aggregates the outputs from the multi-dimensional auditing pipeline into a singular, human-and-machine-readable CSV file.\n",
        "\n",
        "It synthesizes the metadata, the structural static metrics (`LOC`, `Max_CCN`, `SED_Score`), the stylistic adherence scores, and the dynamic runtime profiling results (`CSR_Binary`, `Mem_Safety_Score`, `UB_Status`). This consolidated dataset serves as the direct source of truth for the statistical analyses, tables, and figures presented in the conference paper."
      ],
      "metadata": {
        "id": "3nHjuh6pRUOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- MASTER DATA CONSOLIDATION (CSV) ---\n",
        "# ==========================================\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def consolidate_master_results(dataset, path_folder, filename=\"Phase3_Master_Evaluation_Stats.csv\"):\n",
        "    \"\"\"\n",
        "    Consolidates static, dynamic, and safety metrics into a single CSV.\n",
        "    This creates the primary dataset for the conference paper's statistical analysis.\n",
        "    \"\"\"\n",
        "    print(\"Consolidating all multi-dimensional metrics into a master CSV...\")\n",
        "    final_rows = []\n",
        "\n",
        "    for model in dataset:\n",
        "        for topic in dataset[model]:\n",
        "            for entry in dataset[model][topic]:\n",
        "                # 1. Base Provenance Metadata\n",
        "                row = {\n",
        "                    \"Model\": model,\n",
        "                    \"Topic\": topic,\n",
        "                    \"Iteration\": entry.get(\"iteration\"),\n",
        "                }\n",
        "\n",
        "                # 2. Static Metrics (from Cells 3, 4, 5)\n",
        "                static = entry.get(\"metrics_static\", {})\n",
        "                row.update({\n",
        "                    \"LOC\": static.get(\"loc\", 0),\n",
        "                    \"Max_CCN\": static.get(\"max_ccn\", 0),\n",
        "                    \"Avg_CCN\": static.get(\"avg_ccn\", 0),\n",
        "                    \"Tokens\": static.get(\"tokens\", 0),\n",
        "                    \"Avg_Params\": static.get(\"avg_params\", 0),\n",
        "                    \"Static_Errors\": static.get(\"static_errors\", 0),\n",
        "                    \"SED_Score\": static.get(\"sed_score\", 0),\n",
        "                    \"Style_Deviations\": static.get(\"style_deviations\", 0)\n",
        "                })\n",
        "\n",
        "                # 3. Dynamic & Safety Metrics (from Cells 6, 9, 10)\n",
        "                dyn = entry.get(\"metrics_dynamic\", {})\n",
        "                row.update({\n",
        "                    \"Comp_Status\": dyn.get(\"compilation_status\", \"FAIL\"),\n",
        "                    \"CSR_Binary\": dyn.get(\"csr\", 0),\n",
        "                    \"Valgrind_Status\": dyn.get(\"valgrind_status\", \"N/A\"),\n",
        "                    \"Mem_Safety_Score\": dyn.get(\"mem_safety_score\", 0),\n",
        "                    \"UB_Status\": dyn.get(\"ub_status\", \"N/A\"),\n",
        "                    \"UB_Detail\": dyn.get(\"ub_detail\", \"None\")\n",
        "                })\n",
        "\n",
        "                final_rows.append(row)\n",
        "\n",
        "    # Convert dictionary list to a Pandas DataFrame\n",
        "    df_master = pd.DataFrame(final_rows)\n",
        "\n",
        "    # Save to the Drive folder established in Cell 1\n",
        "    output_path = os.path.join(path_folder, filename)\n",
        "    df_master.to_csv(output_path, index=False)\n",
        "\n",
        "    print(f\"Consolidation Complete. {len(df_master)} rows saved to:\")\n",
        "    print(f\"{output_path}\")\n",
        "\n",
        "    return df_master\n",
        "\n",
        "# Execute Consolidation using the 'path' variable from Cell 1\n",
        "df_final = consolidate_master_results(dataset, path)\n",
        "\n",
        "# ==========================================\n",
        "# --- VALIDATION REPORT OUTPUT ---\n",
        "# ==========================================\n",
        "def verify_master_dataset(df):\n",
        "    print(\"\\nVALIDATION: MASTER DATASET PREVIEW\")\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "    # Group by model to generate a high-level performance snapshot\n",
        "    summary = df.groupby(\"Model\").agg({\n",
        "        \"CSR_Binary\": \"mean\",\n",
        "        \"Mem_Safety_Score\": \"mean\",\n",
        "        \"Max_CCN\": \"mean\",\n",
        "        \"SED_Score\": \"mean\",\n",
        "        \"Avg_CCN\": \"mean\"\n",
        "    }).round(3)\n",
        "\n",
        "    print(\"Aggregated Performance by Model:\")\n",
        "    print(summary)\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "    print(\"Sample Rows (Final 5 in Dataset):\")\n",
        "    # Formatted display of the final rows\n",
        "    print(df.tail(5)[[\"Model\", \"Topic\", \"Iteration\", \"Comp_Status\", \"Valgrind_Status\", \"UB_Status\"]].to_string(index=False))\n",
        "    print(\"-\" * 115)\n",
        "\n",
        "verify_master_dataset(df_final)"
      ],
      "metadata": {
        "id": "znIy0ZSfR0Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Data Visualization: The Reliability Gap (Figure 2a)\n",
        "This cell generates the primary visualization for Research Question 1 (RQ1). It charts the aggregated data from the `df_final` master dataset to illustrate the \"Technical Reliability Gap.\"\n",
        "\n",
        "By plotting the Compilation Success Rate (CSR) side-by-side with the Valgrind Memory Safety Rate, this figure visually demonstrates how surface-level syntactic fluency (compilation) often masks underlying logical and memory management failures in LLM-generated C code."
      ],
      "metadata": {
        "id": "NvzhfmGKR0q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- DATA VISUALIZATION: THE RELIABILITY GAP ---\n",
        "# ==========================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Preparing the data for 4 models\n",
        "gap_df = df_final.groupby(\"Model\")[[\"CSR_Binary\", \"Mem_Safety_Score\"]].mean().reset_index()\n",
        "gap_df = gap_df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Percentage\")\n",
        "\n",
        "# Rename metrics for professional display in the paper\n",
        "gap_df['Metric'] = gap_df['Metric'].replace({\n",
        "    'CSR_Binary': 'Compilation Success (CSR)',\n",
        "    'Mem_Safety_Score': 'Memory Safety (Valgrind)'\n",
        "})\n",
        "\n",
        "# Plotting\n",
        "ax = sns.barplot(data=gap_df, x=\"Model\", y=\"Percentage\", hue=\"Metric\", palette=\"Set2\")\n",
        "plt.title(\"The Reliability Gap: 4-Model Comparison\", fontsize=16, fontweight='bold', pad=20)\n",
        "plt.ylabel(\"Success Rate (0.0 - 1.0)\", fontsize=12)\n",
        "plt.xlabel(\"Model Architecture\", fontsize=12)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.legend(title=\"Metric Type\", loc='upper right')\n",
        "\n",
        "# Annotate values\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fmt='%.2f', padding=3)\n",
        "\n",
        "plt.savefig(\"/content/drive/MyDrive/CANAI_LLM_Results/Eval/Reliability_Gap_4Models.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TNOcld36R_3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Data Visualization: Failure Profile Audit (Figure 2b)\n",
        "\n",
        "This cell generates the secondary visualization for Research Question 1 (RQ1). While the previous chart established the existence of the Reliability Gap, this visualization categorizes the specific technical violations responsible for that gap.\n",
        "\n",
        "By aggregating the string outputs from the Valgrind and UBSan pipelines, this script counts the exact incidence of **Memory Leaks**, **Undefined Behavior**, and **Logical Hangs**. This unmasks the specific failure profiles of each architecture, such as dense models' susceptibility to memory leaks versus their resilience to logical hangs."
      ],
      "metadata": {
        "id": "uKxONpXjSATg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Aggregating failure modes using Named Aggregation\n",
        "# This avoids KeyErrors by explicitly pointing to existing columns first.\n",
        "failure_modes = df_final.groupby(\"Model\").agg(\n",
        "    Memory_Leaks=('Valgrind_Status', lambda x: (x.astype(str).str.strip() == \"SAFETY_VIOLATION\").sum()),\n",
        "    Undefined_Behavior=('UB_Status', lambda x: (x.astype(str).str.strip() == \"UB_TRIGGERED\").sum()),\n",
        "    Logical_Hangs=('Valgrind_Status', lambda x: (x.astype(str).str.strip() == \"TIMEOUT_LOGIC_HANG\").sum())\n",
        ").reset_index()\n",
        "\n",
        "# Rename for clear visualization\n",
        "failure_modes.columns = [\"Model\", \"Memory Leaks\", \"Undefined Behavior\", \"Logical Hangs\"]\n",
        "\n",
        "# 2. Reshape for plotting\n",
        "failure_melted = failure_modes.melt(id_vars=\"Model\", var_name=\"Failure Type\", value_name=\"Total Count\")\n",
        "\n",
        "# 3. Plotting the 4-Model Diagnostic\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "ax = sns.barplot(data=failure_melted, x=\"Model\", y=\"Total Count\", hue=\"Failure Type\", palette=\"magma\")\n",
        "\n",
        "plt.title(\"Failure Profile Audit: Categorizing Technical Violations\", fontsize=16, fontweight='bold', pad=20)\n",
        "plt.ylabel(\"Cumulative Count of Failures\", fontsize=12)\n",
        "plt.xlabel(\"Model Architecture\", fontsize=12)\n",
        "\n",
        "# Adding value labels to the bars\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, padding=3)\n",
        "\n",
        "plt.savefig(\"/content/drive/MyDrive/CANAI_LLM_Results/Eval/Failure_Profiles_Corrected.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ndIsDjq9SRA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. Data Visualization: Complexity Stress Test (Figure 3)\n",
        "This cell generates the categorical distribution chart for the complexity stress test, directly addressing the \"Reliability Ceiling\" discussed in Section 4.2.\n",
        "\n",
        "By grouping the iterations into discrete Cyclomatic Complexity (CCN) bins, this visualization tracks the exact structural density where a model's performance transitions from stability to systemic failure. A vertical threshold marker is injected at $CCN>10$ to highlight the cognitive breaking point where dense generalist models begin to output more memory leaks and logical hangs than safe executions."
      ],
      "metadata": {
        "id": "f4PC7ZhUSRfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- COMPLEXITY STRESS TEST (FIGURE 3) ---\n",
        "# ==========================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define the complexity intervals (bins) and labels\n",
        "bins = [0, 5, 10, 15, 20, 25, 30, 40, 50, 100]\n",
        "labels = ['0-5', '6-10', '11-15', '16-20', '21-25', '26-30', '31-40', '41-50', '51+']\n",
        "\n",
        "# 2. Assign each iteration to an interval in the dataframe\n",
        "df_final['CCN_Interval'] = pd.cut(df_final['Max_CCN'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "# 3. Setting professional academic style and custom palette\n",
        "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
        "custom_palette = {1.0: \"#4C72B0\", 0.0: \"#C44E52\"} # Blue for Pass, Red for Fail\n",
        "\n",
        "# 4. Create the plot using catplot to treat intervals as categories\n",
        "g = sns.catplot(\n",
        "    data=df_final,\n",
        "    x=\"CCN_Interval\",\n",
        "    hue=\"Mem_Safety_Score\",\n",
        "    col=\"Model\",\n",
        "    col_wrap=2,\n",
        "    kind=\"count\",\n",
        "    height=5,\n",
        "    aspect=1.5,\n",
        "    palette=custom_palette,\n",
        "    legend=False\n",
        ")\n",
        "\n",
        "# --- REFINEMENT LOOP ---\n",
        "for ax in g.axes.flat:\n",
        "    # Rotate labels for readability\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.set_xlabel(\"Complexity Interval (Max CCN)\", fontsize=12)\n",
        "    ax.set_ylabel(\"Number of Iterations\", fontsize=12)\n",
        "\n",
        "    # Add a vertical dashed line to mark the 'Reliability Ceiling' (after the 6-10 bin)\n",
        "    ax.axvline(x=1.5, color='black', linestyle='--', linewidth=2, alpha=0.5)\n",
        "\n",
        "# Fix the Legend for conference standard\n",
        "handles = [plt.Rectangle((0,0),1,1, color=custom_palette[1.0]),\n",
        "           plt.Rectangle((0,0),1,1, color=custom_palette[0.0])]\n",
        "g.fig.legend(handles=handles, labels=['PASS (Safe)', 'FAIL (Leak/Hang/UB)'],\n",
        "             title=\"Outcome\", loc='center right', bbox_to_anchor=(1.08, 0.5))\n",
        "\n",
        "g.fig.suptitle(\"Reliability Gap by Complexity Interval: Categorical Distribution\",\n",
        "               fontsize=18, fontweight='bold', y=1.05)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/drive/MyDrive/CANAI_LLM_Results/Eval/Binned_Complexity_Histogram.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NDdOQObcSm7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. Data Visualization: Logic Density (Figure 4a)\n",
        "\n",
        "This cell generates the first half of the Architectural Efficiency analysis (Figure 4a), addressing Research Question 2 (RQ2).\n",
        "\n",
        "To compare the structural characteristics of sparse Mixture-of-Experts (MoE) architectures against dense Transformers, this script calculates a derived metric: **Logic Density**. By dividing the Max Cyclomatic Complexity (CCN) by the total token count and normalizing it per 100 tokens, this violin plot visualizes the distribution of \"information density\" across the generated C solutions."
      ],
      "metadata": {
        "id": "NxHb9EFESnV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- LOGIC DENSITY BY MODEL (VIOLIN PLOT) ---\n",
        "# ==========================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate the derived metric: How much logic is packed into the tokens?\n",
        "df_final['Logic_Density'] = (df_final['Max_CCN'] / df_final['Tokens']) * 100\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Violin plot shows the full distribution 'shape' of logic density\n",
        "sns.violinplot(\n",
        "    data=df_final,\n",
        "    x=\"Model\",\n",
        "    y=\"Logic_Density\",\n",
        "    hue=\"Model\",\n",
        "    palette=\"muted\",\n",
        "    inner=\"quartile\",\n",
        "    legend=False\n",
        ")\n",
        "\n",
        "plt.title(\"Architectural Efficiency: Logic Density (CCN per 100 Tokens)\", fontsize=16, fontweight='bold', pad=20)\n",
        "plt.ylabel(\"Logic Density (Complexity / Tokens * 100)\", fontsize=12)\n",
        "plt.xlabel(\"Model Architecture\", fontsize=12)\n",
        "\n",
        "plt.savefig(\"/content/drive/MyDrive/CANAI_LLM_Results/Eval/Logic_Density_Violin.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XAnhSLT_S3ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. Data Visualization: Model Verbosity and the Safety \"Token Tax\" (Figure 4b)\n",
        "This cell generates the second half of the Architectural Efficiency audit (Figure 4b). It visualizes the total token count distribution across the four evaluated architectures.\n",
        "\n",
        "As analyzed in **Section 4.3**, this data helps differentiate between \"concise\" and \"elaborative\" reasoning models. These results support the finding that Mixture-of-Experts (MoE) architectures consistently expend a higher token budget to implement mandatory safety predicates - such as `malloc` return-value verification - which correlates with their lower incidence of memory leaks compared to more concise, \"short-circuiting\" dense models."
      ],
      "metadata": {
        "id": "KuYwErEoS3-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- MODEL VERBOSITY DISTRIBUTION (FIG 4B) ---\n",
        "# ==========================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Boxplot shows the median, quartiles, and outliers of token counts\n",
        "sns.boxplot(data=df_final, x=\"Model\", y=\"Tokens\", hue=\"Model\", palette=\"Pastel1\", legend=False)\n",
        "\n",
        "plt.title(\"Model Verbosity: Token Distribution per Solution\", fontsize=15, fontweight='bold', pad=20)\n",
        "plt.ylabel(\"Total Token Count\", fontsize=12)\n",
        "plt.xlabel(\"Model Architecture\", fontsize=12)\n",
        "\n",
        "plt.savefig(\"/content/drive/MyDrive/CANAI_LLM_Results/Eval/Verbosity_Boxplot.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yh1oug7XTU96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. Data Visualization: The Style-Safety Paradox (Figure 5)\n",
        "This cell generates the model-specific regression analysis discussed in **Section 5.1**. It investigates whether \"clean\" code‚Äîdefined by low stylistic deviations from the LLVM guide‚Äîcorrelates with higher runtime memory safety.\n",
        "\n",
        "The resulting grid of regressions illustrates the **Style-Safety Paradox**: the finding that for many architectures, there is no significant positive correlation between aesthetic adherence and functional integrity."
      ],
      "metadata": {
        "id": "3JN9I2LZTVc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- STYLE VS. SAFETY REGRESSIONS (FIG 5) ---\n",
        "# ==========================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a 2x2 grid for comparison\n",
        "g = sns.lmplot(\n",
        "    data=df_final,\n",
        "    x=\"Style_Deviations\",\n",
        "    y=\"Mem_Safety_Score\",\n",
        "    col=\"Model\",\n",
        "    hue=\"Model\",\n",
        "    col_wrap=2,\n",
        "    scatter_kws={'alpha':0.2, 's':60},\n",
        "    line_kws={'lw':3},\n",
        "    height=5,\n",
        "    aspect=1.2,\n",
        "    markers=\"x\",\n",
        "    palette=\"deep\"\n",
        ")\n",
        "\n",
        "# Customize titles and labels\n",
        "g.set_axis_labels(\"Style Deviations (Violations)\", \"Memory Safety (1=Pass, 0=Fail)\")\n",
        "g.fig.suptitle(\"Does Style Predict Safety? Model-Specific Regressions\", fontsize=18, fontweight='bold', y=1.05)\n",
        "g.set_titles(\"{col_name}\")\n",
        "\n",
        "plt.savefig(\"/content/drive/MyDrive/CANAI_LLM_Results/Eval/Style_vs_Safety_2x2.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fG0WMy8dTzcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18. Tabular Analysis: Granular Reliability Breakdown (Table 2)\n",
        "This final analytical block produces the numerical data for the study's **Complexity Impact Table**.\n",
        "\n",
        "By calculating the safety percentage across incremental Cyclomatic Complexity (CCN) intervals, this script provides the mathematical proof of architectural performance decay. This granular view is essential for identifying the \"Reliability Ceiling\" - the specific complexity threshold beyond which an architecture can no longer guarantee memory safety or logical termination."
      ],
      "metadata": {
        "id": "mhIGW_tuTz5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- GRANULAR RELIABILITY BREAKDOWN (TABLE 2) ---\n",
        "# ==========================================\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Categorize complexity relative to the Reliability Ceiling\n",
        "df_final['Complexity_Tier'] = df_final['Max_CCN'].apply(lambda x: 'CCN < 10' if x < 10 else 'CCN > 10')\n",
        "\n",
        "# 2. Pivot the data to count Pass/Fail per Model per Tier\n",
        "pivot_table = df_final.groupby(['Model', 'Complexity_Tier', 'Mem_Safety_Score']).size().unstack(fill_value=0)\n",
        "\n",
        "# 3. Renaming columns for academic clarity\n",
        "pivot_table.columns = ['FAIL Count', 'PASS Count']\n",
        "\n",
        "# 4. Flattening the multi-index for a clean display\n",
        "summary_table = pivot_table.reset_index()\n",
        "\n",
        "# 5. Optional: Adding a Success Rate % for extra depth\n",
        "summary_table['Success %'] = (summary_table['PASS Count'] /\n",
        "                             (summary_table['PASS Count'] + summary_table['FAIL Count']) * 100).round(1)\n",
        "\n",
        "# Displaying the final table\n",
        "print(\"--- Technical Breaking Point Summary Table ---\")\n",
        "display(summary_table.sort_values(by=['Complexity_Tier', 'Model']))"
      ],
      "metadata": {
        "id": "pvyxAj7nULxY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}