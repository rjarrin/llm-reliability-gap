{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1: Automated Problem Bank Generation\n",
        "\n",
        "### **Overview**\n",
        "This notebook contains the complete automated pipeline for **Phase 1 (Step 1)** of the methodology. It utilizes three frontier Large Language Models (LLMs) acting as \"curriculum designers\" to collaboratively generate a standardized dataset of C programming problems.\n",
        "\n",
        "### **Methodology**\n",
        "To prevent stylistic bias in the problem descriptions, the generation task is distributed across three high-capacity \"teacher\" models:\n",
        "* **Llama-3.3-70b** (Dense Transformer)\n",
        "* **GPT-OSS 120B** (Sparse Mixture-of-Experts)\n",
        "* **Moonshot Kimi K2** (MoE with Latent Reasoning)\n",
        "\n",
        "Each model generates problems across three core introductory C programming competencies, escalating in cognitive load:\n",
        "1. Pointers and Pointer Arithmetic\n",
        "2. Dynamic Memory Allocation (DMA)\n",
        "3. Data Structures (Singly Linked Lists)\n",
        "\n",
        "**Target Output:** A diverse, mixed-source dataset of 300 unique problems (100 per topic) saved in a standardized format, ready to be ingested by the Phase 2 Audit Pipeline.\n",
        "\n",
        "---\n",
        "**Note for Double-Blind Review:** This codebase has been fully anonymized. Please ensure you provide your own API keys in the environment variables before execution."
      ],
      "metadata": {
        "id": "l4rrpIQ9vrhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Environment Setup & Dependency Installation\n",
        "This cell initializes the Google Colab environment for the generation pipeline. It installs the `groq` client library for high-throughput LLM API access, imports necessary Python modules for JSON parsing and regex pattern matching, and mounts the local Google Drive.\n",
        "\n",
        "Mounting the drive ensures that the generated 300-problem dataset is persistently saved and can be cleanly ingested by the Phase 2 Audit Pipeline.\n",
        "\n",
        "> **Note:** For double-blind execution, you must provide your own Groq API key by replacing the placeholder string below."
      ],
      "metadata": {
        "id": "iBvZ61XJvx-v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3ksUq-4vhV1"
      },
      "outputs": [],
      "source": [
        "# 1. Install the official Groq library\n",
        "!pip install -q groq\n",
        "\n",
        "import os, json, time, re\n",
        "from google.colab import drive\n",
        "from groq import Groq\n",
        "from datetime import datetime\n",
        "\n",
        "# 2. Set API Key (ANONYMIZED FOR DOUBLE-BLIND REVIEW)\n",
        "# Reviewers: Please insert your own Groq API key below.\n",
        "os.environ[\"GROQ_API_KEY\"] = \"YOUR_GROQ_API_KEY_HERE\"\n",
        "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "\n",
        "# 3. Mount Google Drive (for persistent storage of the generated problem bank)\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Phase 1: Problem Bank Generation (Teacher Models)\n",
        "This cell executes **Step 1** of the methodology: generating novel, standardized C programming problems.\n",
        "\n",
        "**Execution Note:** To mitigate Google Colab environment timeouts and prevent data loss during long-running API batches, this script is explicitly designed for modular execution. The researcher selects one `MODEL_NAME` and one `topic_index` per execution block, appending the outputs to a continuous JSONL file.\n",
        "\n",
        "The prompt engineers the model to act as a Computer Science professor, enforcing strict pedagogical constraints (e.g., clear inputs/outputs, mandatory struct usage, and C11 compliance) to ensure high-quality standardized inputs for the Phase 2 audit."
      ],
      "metadata": {
        "id": "Irkavp7Bv4U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- MULTI-TURN GENERATION PIPELINE ---\n",
        "# ==========================================\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Uncomment ONE target model per Colab execution session:\n",
        "# MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
        "MODEL_NAME = \"openai/gpt-oss-120b\"\n",
        "# MODEL_NAME = \"moonshotai/kimi-k2-instruct-0905\"\n",
        "\n",
        "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
        "\n",
        "output_folder = \"/content/drive/MyDrive/CANAI_LLM_Results/\"\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    print(f\"Created directory: {output_folder}\")\n",
        "\n",
        "# 1. TOPIC SELECTION SYSTEM\n",
        "topics = [\n",
        "    \"Pointers and Pointer Arithmetic\",\n",
        "    \"Dynamic Memory Allocation (malloc, free)\",\n",
        "    \"Implementing Data Structures (e.g., Singly Linked Lists)\"\n",
        "]\n",
        "topic_index = 2\n",
        "SELECTED_TOPIC = topics[topic_index]\n",
        "\n",
        "# 2. MANUAL BATCH CONTROL (e.g., run 0-50, then 50-100 to prevent timeouts)\n",
        "start_at = 0\n",
        "end_at = 2\n",
        "\n",
        "# Dynamic filename for safe storage\n",
        "filename = f\"/content/drive/MyDrive/CANAI_LLM_Results/{current_date}_{MODEL_NAME.replace('/', '_')}_{start_at}-{end_at}_{SELECTED_TOPIC.replace(' ', '_')}.jsonl\"\n",
        "\n",
        "# 3. THE 6-STEP PROMPT CHAIN\n",
        "prompts = [\n",
        "    f\"\"\"# STEP 1: PROBLEM\n",
        "    Topic: {SELECTED_TOPIC}\n",
        "    You are a Computer Science professor designing an undergraduate curriculum.\n",
        "    Generate a novel programming problem on {SELECTED_TOPIC}. Your response must begin with the header # STEP 1: PROBLEM.\n",
        "    The problem statement must be clear, unambiguous, and suitable for a student who has just learned this topic.\n",
        "    Requirements:\n",
        "    1. Clear background story or context.\n",
        "    2. A precise list of requirements for the program's functionality.\n",
        "    3. Simple Example of expected Input/Output.\n",
        "    4. An additional constraint, such as 'Must use a struct to represent the primary data entity.', 'Logic for displaying the details of ONE specific entity must be in a function called displayEntity.' or 'The solution must be implemented with a single function besides main()'. The constraint(s) should be listed under the header ### CONSTRAINTS.\n",
        "    5. MANDATORY CONSTRAINTS IF A MENU IS IMPLEMENTED:\n",
        "       - Must include a specific menu option to EXIT the program (clearly state the number or keyword).\"\"\",\n",
        "\n",
        "    f\"\"\"# STEP 2: SOLUTION\n",
        "    Based on the problem you just generated, provide a complete and correct C solution. Your response must begin with the header # STEP 2: SOLUTION.\n",
        "    The code must be well-commented to explain the logic of key sections. It must follow modern C standards (e.g., C11), include all necessary headers, and be formatted for readability.\n",
        "    CRITICAL:\n",
        "    - The code MUST check the return value of all malloc/realloc calls.\n",
        "    - All allocated memory MUST be freed before exit.\n",
        "    - Follow the constraints outlined in the previous STEP 1: PROBLEM statement.\"\"\",\n",
        "\n",
        "    f\"\"\"# STEP 3: EXPLANATION\n",
        "    Regarding the solution code you just provided, write a clear, step-by-step explanation of how it works. Your response must begin with the header # STEP 3: EXPLANATION. Your explanation should be aimed at a student who understands the basic syntax of C but is struggling with {SELECTED_TOPIC}.\n",
        "    Do not just describe what the code does line-by-line; explain the underlying concepts and the 'why' behind the implementation decisions.\"\"\",\n",
        "\n",
        "    f\"\"\"# STEP 4: HINTS\n",
        "    Now, imagine a student is stuck on the original problem you created and has not yet seen the solution.\n",
        "    Generate a series of three progressively more helpful hints to guide them. The hints must not give away the code.\n",
        "    Your response must begin with the header # STEP 4: HINTS.\n",
        "    CRITICAL GUARDRAIL: The hints must not give away any actual C code syntax.\n",
        "    Hint 1: Should be a high-level conceptual nudge about the overall approach.\n",
        "    Hint 2: Should point them toward a specific part of the problem or a key C feature to use.\n",
        "    Hint 3: Should be more direct, suggesting a specific logic structure or the first step to take.\"\"\",\n",
        "\n",
        "    f\"\"\"# STEP 5: SUMMARY\n",
        "    Finally, provide a concise summary of the key learning objectives that this problem-solution pair covers. The summary should be in bullet-point format and highlight the main C programming concepts a student would master by completing this exercise. Your response must begin with the header # STEP 5: SUMMARY.\"\"\",\n",
        "\n",
        "    f\"\"\"# STEP 6: TEST CASES\n",
        "    For the problem you generated, create a comprehensive suite of 5 test cases. Include at least one common case, one edge case (e.g., empty input, null pointer, zero value), and one invalid input case to test the program's error handling. Your response must begin with the header # STEP 6: TEST CASES.\n",
        "\n",
        "    CRITICAL FOR AUTOMATION:\n",
        "    After your descriptions, you MUST provide a machine-readable JSON block\n",
        "    containing the raw strings that a user would type to execute these tests. Ensure newlines within the JSON string are represented as literal '\\\\n' characters and not actual line breaks.\n",
        "\n",
        "    FORMAT:\n",
        "    ```json\n",
        "    {{\n",
        "      \"exit_command\": \"4\",\n",
        "      \"test_suite\": [\n",
        "        {{\"input\": \"1\\\\nJohn\\\\n100\", \"expected_keyword\": \"John\"}},\n",
        "        {{\"input\": \"2\\\\nJohn\", \"expected_keyword\": \"removed\"}}\n",
        "      ]\n",
        "    }}\n",
        "    ```\"\"\"\n",
        "]\n",
        "\n",
        "print(f\"Current Topic: {SELECTED_TOPIC}\")\n",
        "print(f\"Saving results to: {filename}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "# 4. EXECUTION LOOP\n",
        "for i in range(start_at, end_at):\n",
        "    print(f\"\\nSTARTING ITERATION {i+1}...\")\n",
        "\n",
        "    # Structure matches Phase 3 parsing requirements\n",
        "    module_data = {\"iteration\": i+1, \"topic\": SELECTED_TOPIC, \"model\": MODEL_NAME, \"steps\": {}}\n",
        "    history = [{\"role\": \"system\", \"content\": \"You are a CS Professor and Socratic Tutor.\"}]\n",
        "\n",
        "    step = 0\n",
        "    while step < len(prompts):\n",
        "        try:\n",
        "            history.append({\"role\": \"user\", \"content\": prompts[step]})\n",
        "\n",
        "            completion = client.chat.completions.create(\n",
        "                model=MODEL_NAME,\n",
        "                messages=history,\n",
        "                temperature=0.7 # 0.7 ensures novel problem generation across the 100 iterations\n",
        "            )\n",
        "            answer = completion.choices[0].message.content\n",
        "\n",
        "            print(f\"--- Iteration {i+1} | Step {step+1} Generated ---\")\n",
        "            # Preview the response cleanly by removing rapid newlines\n",
        "            preview_text = answer.replace('\\n', ' ')[:150]\n",
        "            print(f\"{preview_text}...\")\n",
        "\n",
        "            # Save step data and append to conversational memory\n",
        "            module_data[\"steps\"][f\"step_{step+1}\"] = answer\n",
        "            history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "            step += 1\n",
        "            time.sleep(12) # Safety buffer to respect API rate limits\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n!!!! API Error: {e}\")\n",
        "            print(\"Action: Stop cell, wait for token reset, and resume from current iteration.\")\n",
        "            raise\n",
        "\n",
        "    # Append the fully generated 6-step iteration to the JSONL file\n",
        "    with open(filename, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(module_data) + \"\\n\")\n",
        "\n",
        "    print(f\"Iteration {i+1} successfully completed and saved.\")"
      ],
      "metadata": {
        "id": "c-qwR7b6wxXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Artifact Generation: Human-Readable Markdown Report\n",
        "While the JSONL format is necessary for the automated ingestion pipeline in Phase 2, this cell generates a supplementary human-readable Markdown (`.md`) report.\n",
        "\n",
        "This allows researchers and peer reviewers to easily spot-check the qualitative aspects of the generated problem bank (e.g., verifying pedagogical constraint adherence and narrative context) without needing to parse the raw data structures."
      ],
      "metadata": {
        "id": "7ABA3FMSwybz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- HUMAN-READABLE REPORT GENERATION ---\n",
        "# ==========================================\n",
        "import json\n",
        "\n",
        "# Sanitize variables for safe file naming\n",
        "safe_model_name = MODEL_NAME.replace('/', '_')\n",
        "safe_topic_name = SELECTED_TOPIC.replace(' ', '_')\n",
        "\n",
        "report_file = f\"/content/drive/MyDrive/CANAI_LLM_Results/Readable_Report_{current_date}_{safe_model_name}_{safe_topic_name}_{start_at}-{end_at}.md\"\n",
        "\n",
        "print(\"Converting raw JSONL data into human-readable Markdown...\")\n",
        "\n",
        "# Convert raw JSONL data into a clean, formatted Markdown file\n",
        "# Added encoding=\"utf-8\" to safely handle LLM-generated special characters\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f, open(report_file, \"w\", encoding=\"utf-8\") as out:\n",
        "    out.write(f\"# C Education Research Report: {SELECTED_TOPIC}\\n\")\n",
        "    out.write(f\"**Model:** {MODEL_NAME} | **Date:** {current_date}\\n\\n\")\n",
        "\n",
        "    for line in f:\n",
        "        try:\n",
        "            data = json.loads(line)\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "\n",
        "        if not data.get(\"steps\"):\n",
        "            continue\n",
        "\n",
        "        out.write(f\"## Iteration {data.get('iteration', 'Unknown')}\\n\")\n",
        "\n",
        "        # Dynamically loops through all available steps (Steps 1-6)\n",
        "        for key, value in data[\"steps\"].items():\n",
        "            # Formats \"step_1\" into \"STEP 1\"\n",
        "            clean_header = key.replace(\"_\", \" \").upper()\n",
        "            out.write(f\"### {clean_header}\\n\")\n",
        "            out.write(f\"{value}\\n\\n\")\n",
        "\n",
        "        out.write(\"---\\n\\n\")\n",
        "\n",
        "print(f\"Readable report successfully created in Drive:\")\n",
        "print(f\"{report_file}\")"
      ],
      "metadata": {
        "id": "jYLlCjuz0YWj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}